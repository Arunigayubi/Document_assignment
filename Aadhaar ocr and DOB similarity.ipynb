{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cccc586-14ea-42ac-a450-c53aed3396e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in /opt/anaconda3/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/anaconda3/lib/python3.12/site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pytesseract) (11.0.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (11.0.0)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.12/site-packages (from opencv-python) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install Pillow\n",
    "!pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73388925-f647-4cc1-904e-83dbc9e51dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbeabb-d966-4654-953f-51dbc2c8df9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8948890c-eef9-4d5d-b6ea-16f2c6092866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easyocr\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting torch (from easyocr)\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchvision>=0.5 (from easyocr)\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting opencv-python-headless (from easyocr)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from easyocr) (1.13.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from easyocr) (2.1.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from easyocr) (10.4.0)\n",
      "Requirement already satisfied: scikit-image in /opt/anaconda3/lib/python3.12/site-packages (from easyocr) (0.24.0)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Downloading python_bidi-0.6.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/lib/python3.12/site-packages (from easyocr) (6.0.1)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Downloading shapely-2.0.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Downloading pyclipper-1.3.0.post6-cp312-cp312-macosx_10_13_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Downloading ninja-1.11.1.2-py3-none-macosx_10_9_universal2.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch->easyocr) (75.1.0)\n",
      "Collecting sympy==1.13.1 (from torch->easyocr)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch->easyocr) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image->easyocr) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image->easyocr) (2023.4.12)\n",
      "Requirement already satisfied: packaging>=21 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image->easyocr) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch->easyocr) (2.1.3)\n",
      "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.2-py3-none-macosx_10_9_universal2.whl (279 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-macosx_10_13_universal2.whl (270 kB)\n",
      "Downloading python_bidi-0.6.3-cp312-cp312-macosx_11_0_arm64.whl (252 kB)\n",
      "Downloading shapely-2.0.6-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, sympy, Shapely, opencv-python-headless, ninja, torch, torchvision, easyocr\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed Shapely-2.0.6 easyocr-1.7.2 ninja-1.11.1.2 opencv-python-headless-4.10.0.84 pyclipper-1.3.0.post6 python-bidi-0.6.3 sympy-1.13.1 torch-2.5.1 torchvision-0.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9edf8520-9b59-40ef-8905-bc4f1d893552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Aadhar number found in the image.\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "def extract_aadhar_number(image_path):\n",
    "    # Step 1: Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])  # Initialize the OCR model with English language\n",
    "\n",
    "    # Step 2: Read the image using OpenCV and pass it to EasyOCR\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Step 3: Use EasyOCR to extract text from the image (NumPy array)\n",
    "    results = reader.readtext(img)\n",
    "\n",
    "    # Step 4: Extract text from the results\n",
    "    extracted_text = ' '.join([text[1] for text in results])\n",
    "\n",
    "    # Step 5: Search for 12-digit Aadhar number in the extracted text\n",
    "    aadhar_number = find_aadhar_number(extracted_text)\n",
    "\n",
    "    return aadhar_number\n",
    "\n",
    "def find_aadhar_number(text):\n",
    "    # Step 6: Regular expression to find a 12-digit number (Aadhar number format)\n",
    "    pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(0)  # Return the found Aadhar number\n",
    "    else:\n",
    "        return None  # Return None if no Aadhar number is found\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/kyc sample.jpeg'  # Replace with your image path\n",
    "aadhar_number = extract_aadhar_number(image_path)\n",
    "\n",
    "if aadhar_number:\n",
    "    print(f\"Aadhar Number Found: {aadhar_number}\")\n",
    "else:\n",
    "    print(\"No Aadhar number found in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ffb3a90-8905-4f8c-b58d-4f40a7570e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: 31THT AADHAAR HRdrt faisiez Bloqtereatut Arirt HRT &aR MeR aarid THIY J18, TR14dd T8. Unique Identifioation Lna € ouundla siorid 5 317alls7 31eT47 E %i &51. Government of India aTa TbTO TerfpRalt MFANTA4TT Enrollment No 1190/11450/02075 5T7 7 1800 180 1947 * 7Td +, fual [tl\"MT 4L, 3, T4 1947, TTd 580001 T45 YTOdT; faval Docmk Vazan Suryu SIo Vagont Shnvo help@uldal gov.In & #at Hanjre Aulq *o InRoam Loundllot DMSingh Roa] Halhibaug Mazqaon T4: 44 HATTTTR TrlR+ Nlumb E Muinb_IE 4Rpcrfd Trre 34r4 067 E. Anhamahke 400440 eraeetl Rcl; Z90 234 / 90679 90694 INSTRUCTIONS Aadhaar proof of identity, note citizenship: Veodi5OTOIDIN To establish identity, authenticate online. In case any help required Call 1800 180 1947 05; Wnte PO. Box No. 1947, Bengaluru 560 001 Or; Email at help@ uidai,gov in M1FM BleR 54ia5 Your Aadhaar No_ Note: Children on attaining years of age 3977 8800 0234 need update biomelnc iniormation: BHR TNT HTNT B*10 28 Hfecertcnli HRclu fafee &iloeuifacbrvr GOVEI UNICUE IdeNMFicaION Ihoaity OF INDIA RttMt Dconik Vaaan Sunvo 9ustt addresse Tu  Yenal pidh TTcTyRt €reh Salarpuria Touchslone 5 / Malo Hreeea Taolc Narathahan Sarjapur; #lbrrI #ronje Ouon Aond WTqa-850087 Bengaluru 560087 3977 8800 0234 Aadhaar Aam Aadmi Ka Adhikaar B1*n HMK4 HTMHTHT Bf4at1r jan SAMPLE Flool, Rg\n",
      "Aadhar Number Found: 3977 8800 0234\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import re\n",
    "\n",
    "def extract_aadhar_number(image_path):\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(image_path)\n",
    "\n",
    "    # Print the extracted text to see what EasyOCR detected\n",
    "    extracted_text = ' '.join([text[1] for text in results])\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "\n",
    "    # Search for 12-digit Aadhar number in the extracted text\n",
    "    aadhar_number = find_aadhar_number(extracted_text)\n",
    "\n",
    "    return aadhar_number\n",
    "\n",
    "def find_aadhar_number(text):\n",
    "    # Regular expression to find a 12-digit number (Aadhar number format)\n",
    "    pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(0)  # Return the found Aadhar number\n",
    "    else:\n",
    "        return None  # Return None if no Aadhar number is found\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/sample aadhaar masking.png'  # Update with the correct image path\n",
    "aadhar_number = extract_aadhar_number(image_path)\n",
    "\n",
    "if aadhar_number:\n",
    "    print(f\"Aadhar Number Found: {aadhar_number}\")\n",
    "else:\n",
    "    print(\"No Aadhar number found in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "490b89fe-4e9d-4959-a709-d245ffe86c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: 18nOnm AADHAAR Adti MRT Har J* 3ir1d 4174 318, Tr14ar 7l, 41oriy Fi #TTTT *e4e1T5ANT tua Tore 4adiehaf ~ila[4ra+r *4i4 Endllmenl No 1190/11450,002075 517 7 1800 189+1047 494} &r, f847 MTTT MTy 4, &, @R4 1847 , #Taa-S80001 & 95 Hlar, f4q4l DocinkVizn Sutu SiO Vneant Sslrvo helpwuldel &ou.In 4 61T Hontc AutahoaRntnDl4enundig @arinmnno Halitn LaTnBrn HTUNTRRTnultfza Numto Muinkxu a1e 4 74747m 54r 3677 5 hahnnhta A0010 Eetetgitx ISA / $0679 90644 INSTRUCTIONS Aaahaar Prool identily , nol - citizenship- Lii  DN To establish Identily, authenticate online; In case any halp required Call 1804 180 1947 or; Wnte PO. Box No. 1947, Bengalunu 580 001 cr; Email nelpguidan gov.in MITAT HFR wiir Your Aarhaar No_ Note; Children an attaining years age 3977 8800 0234 need update Diomelnc Inipimalian: 340r T44g )8 VJ wria faftr &a uneeiu  LVhLaa Wlnuedbncaioh Wnoeity Df INYDIA ZITTCT Te Donpik Viztin $.In\"t 9iat1 51_ Ado(054, (Yalr TaNAR4 7UrN Salarpuria Wucngiona VRIN MaL eLdet TrGr matanana Panapul_ Atangrn #Rongr Ouaoir Ring Hcao Ige 30uo7 5637 3977 8800 0234 Aadhaar Aam Aadmi Ka Adhikaar UNT4 TNONTUT 3646TT Hana _ tlu 0 Fiej, Pangz\n",
      "Aadhar Number Found: 3977 8800 0234\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "def extract_aadhar_number(image_path):\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    # Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Convert the image to grayscale (helps with OCR accuracy)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding to make the text clearer (optional)\n",
    "    _, thresholded_img = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(thresholded_img)\n",
    "\n",
    "    # Print the extracted text to see what EasyOCR detected\n",
    "    extracted_text = ' '.join([text[1] for text in results])\n",
    "    print(\"Extracted Text:\", extracted_text)\n",
    "\n",
    "    # Search for 12-digit Aadhar number in the extracted text\n",
    "    aadhar_number = find_aadhar_number(extracted_text)\n",
    "\n",
    "    return aadhar_number\n",
    "\n",
    "def find_aadhar_number(text):\n",
    "    # Regular expression to find a 12-digit number (Aadhar number format)\n",
    "    pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        return match.group(0)  # Return the found Aadhar number\n",
    "    else:\n",
    "        return None  # Return None if no Aadhar number is found\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/sample aadhaar masking.png'  # Update with the correct image path\n",
    "aadhar_number = extract_aadhar_number(image_path)\n",
    "\n",
    "if aadhar_number:\n",
    "    print(f\"Aadhar Number Found: {aadhar_number}\")\n",
    "else:\n",
    "    print(\"No Aadhar number found in the image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badeb49d-e5cc-4dd2-bdc9-db9ed9fc617d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Extracted Text:\n",
      " HI ITTTT T Name: Adarsh kumar J4an DOB: 17/06/1995 4e2q Male 095237614258 3TTHT 3Taft #I XufT\n",
      "\n",
      "Structured Data:\n",
      "Aadhar Number: Not Found\n",
      "Extracted Text: HI ITTTT T Name Adarsh kumar J4an DOB 17/06/1995 4e2q Male 095237614258 3TTHT 3Taft I XufT\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import re\n",
    "\n",
    "def extract_text(image_path):\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(image_path)\n",
    "    extracted_text = ' '.join([text[1] for text in results])\n",
    "    \n",
    "    print(\"Raw Extracted Text:\\n\", extracted_text)  # Debugging: print raw text\n",
    "    return extracted_text\n",
    "\n",
    "def clean_and_structure_text(raw_text):\n",
    "    # Step 1: Remove unwanted characters and normalize spaces\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', raw_text)  # Replace multiple spaces/newlines with a single space\n",
    "    cleaned_text = re.sub(r'[^\\w\\s/.,-]', '', cleaned_text)  # Remove special characters (retain periods, commas)\n",
    "\n",
    "    # Step 2: Extract Aadhar number (allowing for potential spaces or separators)\n",
    "    aadhar_pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    aadhar_number = re.search(aadhar_pattern, cleaned_text)\n",
    "    aadhar_number = aadhar_number.group(0) if aadhar_number else \"Not Found\"\n",
    "\n",
    "    # Step 3: Identify and structure relevant information (key sections)\n",
    "    structured_data = {\n",
    "        \"Aadhar Number\": aadhar_number,\n",
    "        \"Extracted Text\": cleaned_text,  # Store the cleaned text for context\n",
    "        # Add additional keys if required (like address, name, etc.)\n",
    "    }\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/kyc sample.jpeg'  # Update with the correct image path\n",
    "raw_text = extract_text(image_path)\n",
    "structured_data = clean_and_structure_text(raw_text)\n",
    "\n",
    "# Print structured data\n",
    "print(\"\\nStructured Data:\")\n",
    "for key, value in structured_data.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab5f8dd6-2ea7-4b76-9d4e-fbbcd333019d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Extracted Text:\n",
      " HI ITTTT T Name: Adarsh kumar J4an DOB: 17/06/1995 4e2q Male 095237614258 3TTHT 3Taft #I XufT\n",
      "\n",
      "Extracted Aadhar Details:\n",
      "Aadhar Number: Not Found\n",
      "Name: Adarsh\n",
      "Date of Birth: 17/06/1995\n",
      "Gender: Male\n",
      "PIN Code: Not Found\n",
      "Extracted Text: HI ITTTT T Name Adarsh kumar J4an DOB 17/06/1995 4e2q Male 095237614258 3TTHT 3Taft I XufT\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "import re\n",
    "\n",
    "def extract_text(image_path):\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader(['en'])\n",
    "\n",
    "    # Extract text using EasyOCR\n",
    "    results = reader.readtext(image_path)\n",
    "    extracted_text = ' '.join([text[1] for text in results])\n",
    "    print(\"Raw Extracted Text:\\n\", extracted_text)  # Debugging: print raw text\n",
    "    return extracted_text\n",
    "\n",
    "def extract_aadhar_details(raw_text):\n",
    "    # Step 1: Clean and normalize the text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', raw_text)  # Replace multiple spaces/newlines with a single space\n",
    "    cleaned_text = re.sub(r'[^\\w\\s/.,-]', '', cleaned_text)  # Remove special characters except periods, commas\n",
    "\n",
    "    # Step 2: Extract Aadhar number (allow spaces or separators)\n",
    "    aadhar_pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    aadhar_number = re.search(aadhar_pattern, cleaned_text)\n",
    "    aadhar_number = aadhar_number.group(0) if aadhar_number else \"Not Found\"\n",
    "\n",
    "    # Step 3: Extract name (look for \"Name\" or assume it's a capitalized text block)\n",
    "    name_pattern = r'Name:?\\s([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)'\n",
    "    name_match = re.search(name_pattern, cleaned_text)\n",
    "    name = name_match.group(1) if name_match else \"Not Found\"\n",
    "\n",
    "    # Step 4: Extract Date of Birth\n",
    "    dob_pattern = r'\\d{2}/\\d{2}/\\d{4}'  # Matches \"DD/MM/YYYY\"\n",
    "    dob_match = re.search(dob_pattern, cleaned_text)\n",
    "    dob = dob_match.group(0) if dob_match else \"Not Found\"\n",
    "\n",
    "    # Step 5: Extract Gender\n",
    "    gender = \"Male\" if \"Male\" in cleaned_text else \"Female\" if \"Female\" in cleaned_text else \"Not Found\"\n",
    "\n",
    "    # Step 6: Extract Address (basic assumption based on keywords like \"Address\" or PIN code pattern)\n",
    "    pin_pattern = r'\\b\\d{6}\\b'  # Indian PIN code (6 digits)\n",
    "    pin_match = re.search(pin_pattern, cleaned_text)\n",
    "    pin_code = pin_match.group(0) if pin_match else \"Not Found\"\n",
    "\n",
    "    # Compile structured details\n",
    "    structured_data = {\n",
    "        \"Aadhar Number\": aadhar_number,\n",
    "        \"Name\": name,\n",
    "        \"Date of Birth\": dob,\n",
    "        \"Gender\": gender,\n",
    "        \"PIN Code\": pin_code,\n",
    "        \"Extracted Text\": cleaned_text,  # For debugging or context\n",
    "    }\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/kyc sample.jpeg'  # Update with the correct image path\n",
    "raw_text = extract_text(image_path)\n",
    "aadhar_details = extract_aadhar_details(raw_text)\n",
    "\n",
    "# Print structured data\n",
    "print(\"\\nExtracted Aadhar Details:\")\n",
    "for key, value in aadhar_details.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1f0aade-a8e2-410f-9fb0-7e50dd34fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Set up Tesseract executable path for Mac/Linux (not needed if already in PATH)\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/opt/homebrew/bin/tesseract\" # Update if installed elsewhere\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281819cf-cc44-4243-bd24-4c60a740c3ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Extracted Text:\n",
      " oun weet\n",
      "\n",
      "am / Name:\n",
      "\n",
      "Adarsh kumar\n",
      "\n",
      "wera / DOB: 17/06/1995\n",
      "Gra / Male\n",
      "\n",
      "095237614258\n",
      "\n",
      "Sey - aaah wr waka\n",
      "\n",
      "\n",
      "\n",
      "Extracted Aadhar Details:\n",
      "Aadhar Number: Not Found\n",
      "Name: Adarsh\n",
      "Date of Birth: 17/06/1995\n",
      "Gender: Male\n",
      "PIN Code: Not Found\n",
      "Extracted Text: oun weet am / Name Adarsh kumar wera / DOB 17/06/1995 Gra / Male 095237614258 Sey - aaah wr waka \n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "# Set up Tesseract executable path for Mac/Linux (not needed if already in PATH)\n",
    "pytesseract.pytesseract.tesseract_cmd = \"/opt/homebrew/bin/tesseract\"  # Update if installed elsewhere\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    # Step 1: Read the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Step 2: Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Step 3: Apply thresholding to improve text visibility\n",
    "    _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    return thresh\n",
    "\n",
    "def extract_text_with_tesseract(image):\n",
    "    # Use Tesseract to extract text from the preprocessed image\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    print(\"Raw Extracted Text:\\n\", extracted_text)  # Debugging: print raw text\n",
    "    return extracted_text\n",
    "\n",
    "def extract_aadhar_details(raw_text):\n",
    "    # Step 1: Clean and normalize the text\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', raw_text)  # Replace multiple spaces/newlines with a single space\n",
    "    cleaned_text = re.sub(r'[^\\w\\s/.,-]', '', cleaned_text)  # Remove special characters except periods, commas\n",
    "\n",
    "    # Step 2: Extract Aadhar number (allow spaces or separators)\n",
    "    aadhar_pattern = r'\\b\\d{4}\\s\\d{4}\\s\\d{4}\\b'\n",
    "    aadhar_number = re.search(aadhar_pattern, cleaned_text)\n",
    "    aadhar_number = aadhar_number.group(0) if aadhar_number else \"Not Found\"\n",
    "\n",
    "    # Step 3: Extract name (look for \"Name\" or assume it's a capitalized text block)\n",
    "    name_pattern = r'Name:?\\s([A-Z][a-z]+(?:\\s[A-Z][a-z]+)*)'\n",
    "    name_match = re.search(name_pattern, cleaned_text)\n",
    "    name = name_match.group(1) if name_match else \"Not Found\"\n",
    "\n",
    "    # Step 4: Extract Date of Birth\n",
    "    dob_pattern = r'\\d{2}/\\d{2}/\\d{4}'  # Matches \"DD/MM/YYYY\"\n",
    "    dob_match = re.search(dob_pattern, cleaned_text)\n",
    "    dob = dob_match.group(0) if dob_match else \"Not Found\"\n",
    "\n",
    "    # Step 5: Extract Gender\n",
    "    gender = \"Male\" if \"Male\" in cleaned_text else \"Female\" if \"Female\" in cleaned_text else \"Not Found\"\n",
    "\n",
    "    # Step 6: Extract Address (basic assumption based on keywords like \"Address\" or PIN code pattern)\n",
    "    pin_pattern = r'\\b\\d{6}\\b'  # Indian PIN code (6 digits)\n",
    "    pin_match = re.search(pin_pattern, cleaned_text)\n",
    "    pin_code = pin_match.group(0) if pin_match else \"Not Found\"\n",
    "\n",
    "    # Compile structured details\n",
    "    structured_data = {\n",
    "        \"Aadhar Number\": aadhar_number,\n",
    "        \"Name\": name,\n",
    "        \"Date of Birth\": dob,\n",
    "        \"Gender\": gender,\n",
    "        \"PIN Code\": pin_code,\n",
    "        \"Extracted Text\": cleaned_text,  # For debugging or context\n",
    "    }\n",
    "\n",
    "    return structured_data\n",
    "\n",
    "# Example usage\n",
    "image_path = '/Users/aruniga.baskaran/Downloads/kyc sample.jpeg'  # Update with the correct image path\n",
    "processed_image = preprocess_image(image_path)\n",
    "raw_text = extract_text_with_tesseract(processed_image)\n",
    "aadhar_details = extract_aadhar_details(raw_text)\n",
    "\n",
    "# Print structured data\n",
    "print(\"\\nExtracted Aadhar Details:\")\n",
    "for key, value in aadhar_details.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44ba4729-75b7-41f1-b1af-7daed0e0851f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.0.0\n",
      "Uninstalling numpy-2.0.0:\n",
      "  Successfully uninstalled numpy-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53f866ff-e02e-41b7-9bc6-c8dbdf558fab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Installing collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.0.0 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13dc377-79b1-4a74-a148-e11f1518b7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.0 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for numpy==1.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e49da110-4ec6-4180-9e29-34d10b596df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting easyocr\n",
      "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting torch (from easyocr)\n",
      "  Downloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting torchvision>=0.5 (from easyocr)\n",
      "  Downloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Collecting opencv-python-headless (from easyocr)\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting scipy (from easyocr)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting numpy (from easyocr)\n",
      "  Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting Pillow (from easyocr)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting scikit-image (from easyocr)\n",
      "  Downloading scikit_image-0.24.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Collecting python-bidi (from easyocr)\n",
      "  Downloading python_bidi-0.6.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting PyYAML (from easyocr)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting Shapely (from easyocr)\n",
      "  Downloading shapely-2.0.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.0 kB)\n",
      "Collecting pyclipper (from easyocr)\n",
      "  Downloading pyclipper-1.3.0.post6-cp312-cp312-macosx_10_13_universal2.whl.metadata (9.0 kB)\n",
      "Collecting ninja (from easyocr)\n",
      "  Downloading ninja-1.11.1.2-py3-none-macosx_10_9_universal2.whl.metadata (5.3 kB)\n",
      "Collecting packaging>=21.3 (from pytesseract)\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting filelock (from torch->easyocr)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch->easyocr)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting networkx (from torch->easyocr)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch->easyocr)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch->easyocr)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch->easyocr)\n",
      "  Downloading setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch->easyocr)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->easyocr)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image->easyocr)\n",
      "  Downloading imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image->easyocr)\n",
      "  Downloading tifffile-2024.9.20-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image->easyocr)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->easyocr)\n",
      "  Downloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m863.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp312-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.2-py3-none-macosx_10_9_universal2.whl (279 kB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp312-cp312-macosx_10_13_universal2.whl (270 kB)\n",
      "Downloading python_bidi-0.6.3-cp312-cp312-macosx_11_0_arm64.whl (252 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading scikit_image-0.24.0-cp312-cp312-macosx_12_0_arm64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.14.1-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.6-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio-2.36.1-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.9.20-py3-none-any.whl (228 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, mpmath, typing-extensions, sympy, setuptools, PyYAML, Pillow, packaging, numpy, ninja, networkx, MarkupSafe, fsspec, filelock, tifffile, Shapely, scipy, pytesseract, opencv-python-headless, lazy-loader, jinja2, imageio, torch, scikit-image, torchvision, easyocr\n",
      "  Attempting uninstall: python-bidi\n",
      "    Found existing installation: python-bidi 0.6.3\n",
      "    Uninstalling python-bidi-0.6.3:\n",
      "      Successfully uninstalled python-bidi-0.6.3\n",
      "  Attempting uninstall: pyclipper\n",
      "    Found existing installation: pyclipper 1.3.0.post6\n",
      "    Uninstalling pyclipper-1.3.0.post6:\n",
      "      Successfully uninstalled pyclipper-1.3.0.post6\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.1\n",
      "    Uninstalling sympy-1.13.1:\n",
      "      Successfully uninstalled sympy-1.13.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.1.0\n",
      "    Uninstalling setuptools-75.1.0:\n",
      "      Successfully uninstalled setuptools-75.1.0\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: ninja\n",
      "    Found existing installation: ninja 1.11.1.2\n",
      "    Uninstalling ninja-1.11.1.2:\n",
      "      Successfully uninstalled ninja-1.11.1.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.3\n",
      "    Uninstalling networkx-3.3:\n",
      "      Successfully uninstalled networkx-3.3\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.3\n",
      "    Uninstalling MarkupSafe-2.1.3:\n",
      "      Successfully uninstalled MarkupSafe-2.1.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: tifffile\n",
      "    Found existing installation: tifffile 2023.4.12\n",
      "    Uninstalling tifffile-2023.4.12:\n",
      "      Successfully uninstalled tifffile-2023.4.12\n",
      "  Attempting uninstall: Shapely\n",
      "    Found existing installation: shapely 2.0.6\n",
      "    Uninstalling shapely-2.0.6:\n",
      "      Successfully uninstalled shapely-2.0.6\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "  Attempting uninstall: pytesseract\n",
      "    Found existing installation: pytesseract 0.3.13\n",
      "    Uninstalling pytesseract-0.3.13:\n",
      "      Successfully uninstalled pytesseract-0.3.13\n",
      "  Attempting uninstall: opencv-python-headless\n",
      "    Found existing installation: opencv-python-headless 4.10.0.84\n",
      "    Uninstalling opencv-python-headless-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-headless-4.10.0.84\n",
      "  Attempting uninstall: lazy-loader\n",
      "    Found existing installation: lazy_loader 0.4\n",
      "    Uninstalling lazy_loader-0.4:\n",
      "      Successfully uninstalled lazy_loader-0.4\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: imageio\n",
      "    Found existing installation: imageio 2.33.1\n",
      "    Uninstalling imageio-2.33.1:\n",
      "      Successfully uninstalled imageio-2.33.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1\n",
      "    Uninstalling torch-2.5.1:\n",
      "      Successfully uninstalled torch-2.5.1\n",
      "  Attempting uninstall: scikit-image\n",
      "    Found existing installation: scikit-image 0.24.0\n",
      "    Uninstalling scikit-image-0.24.0:\n",
      "      Successfully uninstalled scikit-image-0.24.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.20.1\n",
      "    Uninstalling torchvision-0.20.1:\n",
      "      Successfully uninstalled torchvision-0.20.1\n",
      "  Attempting uninstall: easyocr\n",
      "    Found existing installation: easyocr 1.7.2\n",
      "    Uninstalling easyocr-1.7.2:\n",
      "      Successfully uninstalled easyocr-1.7.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.1.3 which is incompatible.\n",
      "gensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.14.1 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.1.3 which is incompatible.\n",
      "s3fs 2024.6.1 requires fsspec==2024.6.1.*, but you have fsspec 2024.10.0 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.0.0 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 Pillow-11.0.0 PyYAML-6.0.2 Shapely-2.0.6 easyocr-1.7.2 filelock-3.16.1 fsspec-2024.10.0 imageio-2.36.1 jinja2-3.1.4 lazy-loader-0.4 mpmath-1.3.0 networkx-3.4.2 ninja-1.11.1.2 numpy-2.1.3 opencv-python-headless-4.10.0.84 packaging-24.2 pyclipper-1.3.0.post6 pytesseract-0.3.13 python-bidi-0.6.3 scikit-image-0.24.0 scipy-1.14.1 setuptools-75.6.0 sympy-1.13.1 tifffile-2024.9.20 torch-2.5.1 torchvision-0.20.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall easyocr pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81aeba2f-1f38-4a5c-883d-9d6d421298f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy -U\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dc42979-bc96-4532-9c35-e51c49c07766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: 2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy<2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4970455-20ee-4102-af05-4a475f47475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy easyocr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61928e1d-935f-4b32-894f-b739dd60ea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#next task - masking date of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7ca474-42a0-4b79-b75d-c62e5e69591f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar (masked): Another date: NN-NN-NNNN\n",
      "Original text: Another date: 12-05-1985\n",
      "Similarity score: 0.6756\n",
      "Extracted DOB: ['12-05-1985']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Define a function to mask DOB patterns\n",
    "def mask_dob(text):\n",
    "    dob_pattern = r'\\b(\\d{2}-\\d{2}-\\d{4})\\b'  # Pattern for dd-mm-yyyy\n",
    "    return re.sub(dob_pattern, 'NN-NN-NNNN', text)\n",
    "\n",
    "# Step 2: Define a function to validate and extract DOBs\n",
    "def extract_dob(text):\n",
    "    dob_pattern = r'\\b(\\d{2}-\\d{2}-\\d{4})\\b'  # Pattern for dd-mm-yyyy\n",
    "    matches = re.findall(dob_pattern, text)\n",
    "    return matches\n",
    "\n",
    "# Step 3: Compute similarity and find the best match\n",
    "def find_similar_pattern(input_pattern, documents):\n",
    "    # Mask the documents\n",
    "    masked_documents = [mask_dob(doc) for doc in documents]\n",
    "    \n",
    "    # Vectorize using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([input_pattern] + masked_documents)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
    "    \n",
    "    # Identify the most similar document\n",
    "    most_similar_idx = cosine_similarities.argmax()\n",
    "    return masked_documents[most_similar_idx], documents[most_similar_idx], cosine_similarities[most_similar_idx]\n",
    "\n",
    "# Example usage\n",
    "documents = [\n",
    "    \"Aadhaar DOB: 21-06-1990\",\n",
    "    \"Another date: 12-05-1985\",\n",
    "    \"Random text with 23-11-2000 as a date\"\n",
    "]\n",
    "\n",
    "input_pattern = \"NN-NN-NNNN\"  # Masked form of the desired pattern\n",
    "\n",
    "# Find the most similar document\n",
    "masked_match, original_match, similarity_score = find_similar_pattern(input_pattern, documents)\n",
    "\n",
    "# Extract actual DOB from the best match\n",
    "extracted_dob = extract_dob(original_match)\n",
    "\n",
    "# Output results\n",
    "print(f\"Most similar (masked): {masked_match}\")\n",
    "print(f\"Original text: {original_match}\")\n",
    "print(f\"Similarity score: {similarity_score:.4f}\")\n",
    "print(f\"Extracted DOB: {extracted_dob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916f529f-33b6-479b-b12c-3dcb8a42a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar (masked): Aadhaar DOB: NN-NN-NNNN\n",
      "Original text: Aadhaar DOB: 21-06-1990\n",
      "Similarity score: 0.7001\n",
      "Extracted DOB: ['21-06-1990']\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Extract text from image using OCR\n",
    "def extract_text_from_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text\n",
    "\n",
    "# Step 2: Mask DOB patterns\n",
    "def mask_dob(text):\n",
    "    dob_pattern = r'\\b(\\d{1,2}\\?*-\\d{1,2}\\?*-\\d{4}\\?*)\\b'  # Allows missing numbers with '?'\n",
    "    return re.sub(dob_pattern, 'NN-NN-NNNN', text)\n",
    "\n",
    "# Step 3: Validate and extract DOBs\n",
    "def extract_dob(text):\n",
    "    dob_pattern = r'\\b(\\d{1,2}\\?*-\\d{1,2}\\?*-\\d{4}\\?*)\\b'  # Matches DOBs with potential missing numbers\n",
    "    return re.findall(dob_pattern, text)\n",
    "\n",
    "# Step 4: Compute similarity and find matches\n",
    "def find_similar_pattern(input_pattern, documents):\n",
    "    # Mask the documents\n",
    "    masked_documents = [mask_dob(doc) for doc in documents]\n",
    "    \n",
    "    # Vectorize using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform([input_pattern] + masked_documents)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
    "    \n",
    "    # Identify the most similar document\n",
    "    most_similar_idx = cosine_similarities.argmax()\n",
    "    return masked_documents[most_similar_idx], documents[most_similar_idx], cosine_similarities[most_similar_idx]\n",
    "\n",
    "# Example usage\n",
    "image_path = \"/Users/aruniga.baskaran/Downloads/kyc sample.jpeg\"  # Replace with your Aadhaar image path\n",
    "\n",
    "# Extract text from the Aadhaar image\n",
    "extracted_text = extract_text_from_image(image_path)\n",
    "\n",
    "# Example documents (including extracted text)\n",
    "documents = [\n",
    "    extracted_text,\n",
    "    \"Aadhaar DOB: 21-06-1990\",\n",
    "    \"Another date: 12-05-1985\",\n",
    "    \"Random text with 23-11-2000 as a date\"\n",
    "]\n",
    "\n",
    "input_pattern = \"NN-NN-NNNN\"  # Masked form of the desired pattern\n",
    "\n",
    "# Find the most similar document\n",
    "masked_match, original_match, similarity_score = find_similar_pattern(input_pattern, documents)\n",
    "\n",
    "# Extract actual DOB from the best match\n",
    "extracted_dob = extract_dob(original_match)\n",
    "\n",
    "# Output results\n",
    "print(f\"Most similar (masked): {masked_match}\")\n",
    "print(f\"Original text: {original_match}\")\n",
    "print(f\"Similarity score: {similarity_score:.4f}\")\n",
    "print(f\"Extracted DOB: {extracted_dob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7e66622c-0a3a-4cc7-954c-b58ebdde3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## masking the dob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9ac57944-d9f7-40d2-863d-de18781d1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text: art / Name:\n",
      "Adarsh kumar\n",
      "\n",
      "wT ante / DOB: 17/06/1995\n",
      "Yeu / Male\n",
      "\n",
      "095237614258\n",
      "\n",
      "amen - area ar rufa\n",
      "\n",
      "\n",
      "Extracted DOB: 17/06/1995\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pytesseract\n",
    "\n",
    "# Step 1: Extract Date of Birth from Image\n",
    "def extract_dob_from_image(image_path):\n",
    "    text = pytesseract.image_to_string(image_path)  # OCR Extraction\n",
    "    print(\"Extracted Text:\", text)  # Debugging line to see the OCR output\n",
    "    dob_pattern = r'\\b\\d{2}[/\\-]\\d{2}[/\\-]?\\d{2,4}\\b'  # Pattern: DD/MM/YY or DD/MM/YYYY\n",
    "    matches = re.findall(dob_pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        return matches[0]  # Return the first matched date\n",
    "    else:\n",
    "        print(\"No date found in the image text.\")\n",
    "        return None\n",
    "\n",
    "# Test the function with an image path\n",
    "image_path = \"/Users/aruniga.baskaran/Downloads/kyc sample.jpeg\"\n",
    "dob = extract_dob_from_image(image_path)\n",
    "if dob:\n",
    "    print(\"Extracted DOB:\", dob)\n",
    "else:\n",
    "    print(\"Date of Birth not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "316ed21e-8b0e-4751-b824-c3ae3419dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number number special number number special number number number number\n"
     ]
    }
   ],
   "source": [
    "# Mask the date of birth\n",
    "masked_dob = ' '.join(['number' if c.isdigit() else 'special' for c in dob])\n",
    "print(masked_dob)  # e.g., \"number number special number number special number\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3b25bc5c-8012-4d8a-b200-f07e3589f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of DOB strings in the corpus\n",
    "corpus = [\n",
    "    \"20/03/98\",\n",
    "    \"12/04\",\n",
    "    \"05/09/1984\"\n",
    "]\n",
    "\n",
    "# Function to mask a DOB string (replace digits with 'number' and other characters with 'special')\n",
    "def mask_dob(dob):\n",
    "    return ' '.join(['number' if c.isdigit() else 'special' for c in dob])\n",
    "\n",
    "# Loop over each DOB in the corpus and mask it\n",
    "masked_corpus = [mask_dob(dob) for dob in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9d8ec8fd-dee1-4a60-b8a1-49c9671ebb1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number number special number number special number number',\n",
       " 'number number special number number',\n",
       " 'number number special number number special number number number number']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6af9bf6c-9be9-4958-a363-41edaab1b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute the TF-IDF scores for the tokens in the corpus\n",
    "  # In this example, we only have one DOB string, but this can be expanded\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(masked_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4d9ec7a-9b16-4a59-a981-b44559e49cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of tokens (which are 'number' and 'special' in the masked DOB)\n",
    "tokens = masked_dob.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "432e9d64-4dfc-4e57-ab75-db6add95adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping each token to its TF-IDF score from the corpus\n",
    "tfidf_scores = {}\n",
    "for token in tokens:\n",
    "    if token in vectorizer.get_feature_names_out():\n",
    "        idx = vectorizer.get_feature_names_out().tolist().index(token)\n",
    "        tfidf_scores[token] = X[0, idx]  # Get the TF-IDF score for that token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eab1461c-0cab-4571-8236-447c9ae7a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert the masked DOB string to spaCy token embeddings\n",
    "masked_dob_doc = nlp(masked_dob)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c91e37ae-a914-421a-8c16-79c721cec5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Get the embeddings for 'number' and 'special' tokens and weight them by TF-IDF scores\n",
    "number_embeddings = []\n",
    "special_embeddings = []\n",
    "\n",
    "for token in masked_dob_doc:\n",
    "    if token.text == \"number\" and \"number\" in tfidf_scores:\n",
    "        number_embeddings.append(token.vector * tfidf_scores[\"number\"])\n",
    "    elif token.text == \"special\" and \"special\" in tfidf_scores:\n",
    "        special_embeddings.append(token.vector * tfidf_scores[\"special\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a96f3ae-a329-4247-84e8-d8de4db06870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Average pooling for 'number' and 'special' embeddings\n",
    "def average_pooling(embeddings):\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros_like(embeddings[0])  # Return a zero vector if no embeddings found\n",
    "\n",
    "# Calculate the average vector for 'number' and 'special'\n",
    "number_avg_vector = average_pooling(number_embeddings)\n",
    "special_avg_vector = average_pooling(special_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ac06ad4-e49f-44dc-8e5a-0385c348913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Vector Representation of the DOB (weighted by TF-IDF):\n",
      "[ 3.52184057e-01 -6.41880810e-01  4.51741755e-01  5.21241069e-01\n",
      "  1.41846135e-01 -4.66189891e-01  1.61164379e+00  9.84569550e-01\n",
      "  1.47740960e-01  8.17762464e-02 -3.45037371e-01 -4.94781911e-01\n",
      " -2.34671220e-01  2.79526263e-01 -1.23239249e-01  6.57517791e-01\n",
      " -6.68680906e-01 -3.67707849e-01 -5.88503778e-02 -6.75583243e-01\n",
      " -3.86384785e-01  7.71861076e-01 -2.27449968e-01 -1.13073200e-01\n",
      "  7.85142034e-02 -3.98744270e-02 -1.54981852e-01 -3.79504383e-01\n",
      "  5.41910589e-01  3.88820976e-01 -3.87908727e-01 -1.70446172e-01\n",
      " -3.45728159e-01  1.10264413e-01 -6.14757895e-01 -3.53684992e-01\n",
      " -2.54599992e-02 -5.34719944e-01  1.61628515e-01 -2.16504931e-01\n",
      " -1.75936565e-01  1.06286120e+00  2.64554203e-01  8.95001411e-01\n",
      " -1.39438286e-01  3.19494307e-01 -1.31132865e+00 -2.52340198e-01\n",
      "  1.70524180e-01 -1.07711375e-01 -2.49185786e-03 -4.01716918e-01\n",
      "  5.81399620e-01 -8.10489774e-01  8.02200079e-01 -9.93070662e-01\n",
      "  4.94256914e-01 -4.93992567e-01 -2.82281429e-01 -6.09081924e-01\n",
      " -6.39047921e-01  2.47330517e-01  1.68659210e-01 -7.21106410e-01\n",
      " -5.35981417e-01 -5.99744201e-01 -1.79231316e-01  3.44898164e-01\n",
      "  1.46734059e-01 -4.52508807e-01  1.00000672e-01  3.52365613e-01\n",
      "  6.51104867e-01 -4.84535366e-01  9.81153250e-01  7.48429835e-01\n",
      "  6.67174309e-02 -2.70561606e-01  7.47211218e-01 -3.53859901e-01\n",
      " -1.95312351e-01 -4.78490144e-01  8.85761619e-01 -4.94131476e-01\n",
      " -3.91741991e-01  4.04080898e-01  9.05484855e-02 -2.54526198e-01\n",
      " -5.07152617e-01  6.45680487e-01 -6.47285342e-01  8.64178538e-02\n",
      "  8.99607778e-01  2.27748305e-01 -2.48581767e-01  4.17653233e-01\n",
      "  2.99096942e-01 -1.05042219e-01 -1.39004588e-01  5.95405092e-03\n",
      " -3.07089150e-01  1.43387303e-01  8.33041072e-02  9.46294218e-02\n",
      " -9.26777869e-02 -1.45387053e-01 -5.06900325e-02  2.05765620e-01\n",
      " -1.99831024e-01 -1.28463030e-01 -2.76549786e-01  9.62198600e-02\n",
      " -4.70076740e-01 -1.68778867e-01 -6.11403435e-02 -1.10612631e-01\n",
      " -2.33673841e-01  1.89216375e-01 -1.82268679e-01  1.91375762e-02\n",
      "  1.17003918e-02 -3.80756259e-01 -1.64335191e-01 -1.19318515e-02\n",
      "  3.91723931e-01 -2.00915366e-01  6.27247244e-02  1.08051943e-02\n",
      " -8.87033343e-02  2.78473590e-02 -7.91082531e-03 -2.22825445e-02\n",
      " -6.13627434e-02  2.58682761e-02  1.01319969e-01 -1.88149065e-01\n",
      " -2.05978543e-01  5.06769180e-01 -4.74369973e-02  2.30360880e-01\n",
      "  1.12092078e-01 -6.21827357e-02 -4.02188897e-01  1.15100756e-01\n",
      "  4.75828499e-02  4.43888977e-02 -2.32415684e-02  1.78957611e-01\n",
      " -3.15242931e-02 -4.33937833e-03  4.02867794e-02  3.70843112e-01\n",
      "  1.05215900e-01  1.21597603e-01 -2.26188704e-01  2.99730986e-01\n",
      " -2.05156684e-01 -1.58638701e-01 -1.23733133e-01 -1.31394118e-01\n",
      " -3.04555297e-01 -2.45819286e-01  1.60073251e-01  1.68741435e-01\n",
      "  2.21547056e-02  2.96741445e-03  7.01821893e-02  1.00291960e-01\n",
      " -2.57209569e-01 -1.93219602e-01  2.71539807e-01  1.99804306e-02\n",
      " -5.69134131e-02 -1.09180529e-03  5.93546778e-02  1.18265942e-01\n",
      "  1.70990855e-01 -1.90107256e-01  2.11350650e-01  6.45333156e-02\n",
      " -3.83074522e-01  2.71029323e-02  1.79385170e-01  3.19046751e-02\n",
      " -1.22837394e-01  2.42124088e-02  2.52324671e-01  4.72886801e-01\n",
      "  1.57923564e-01  2.28754103e-01 -3.24913561e-01  2.56043989e-02]\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Combine the two average vectors (you could also concatenate them)\n",
    "combined_vector = np.concatenate([number_avg_vector, special_avg_vector])\n",
    "\n",
    "# Print the final combined vector (summarized representation of the entire DOB)\n",
    "print(\"Combined Vector Representation of the DOB (weighted by TF-IDF):\")\n",
    "print(combined_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "724b8a04-91ea-48d5-8c35-38bcba81876e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_combined_vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m all_combined_vectors\u001b[38;5;241m.\u001b[39mappend(combined_vector)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Step 7: Compute the cosine similarity matrix between all combined vectors\u001b[39;00m\n\u001b[1;32m      4\u001b[0m similarity_matrix \u001b[38;5;241m=\u001b[39m cosine_similarity(all_combined_vectors)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_combined_vectors' is not defined"
     ]
    }
   ],
   "source": [
    "all_combined_vectors.append(combined_vector)\n",
    "\n",
    "# Step 7: Compute the cosine similarity matrix between all combined vectors\n",
    "similarity_matrix = cosine_similarity(all_combined_vectors)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d315a-c43e-47ec-8b18-bb09fbab94a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91445618-8b4a-49b5-987c-69da8acc4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Example list of DOB strings (you can expand or replace these with more examples)\n",
    "dob_strings = [\n",
    "    \"20/03/98\",\n",
    "    \"12/04\",\n",
    "    \"05/09/1984\"\n",
    "]\n",
    "\n",
    "# Step 1: Calculate the TF-IDF scores for the tokens in the corpus\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(dob_strings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7a9884-1a33-435b-85aa-cf0366ea173d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f3901-0c51-431d-ac97-b5d65a42dbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b3858-6445-4321-9912-deb43a0bf2bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ab4b890-8b8a-495e-b3b9-6123c041657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.0000001 0.9946969 0.9961081 0.9946969]\n",
      " [0.9946969 1.0000001 0.9880272 1.0000001]\n",
      " [0.9961081 0.9880272 1.0000001 0.9880272]\n",
      " [0.9946969 1.0000001 0.9880272 1.0000001]]\n",
      "DOB '20/03/98' and DOB '12/04' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '05/09/1984' are similar (cosine similarity: 1.00)\n",
      "DOB '20/03/98' and DOB '45/67' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '05/09/1984' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '45/67' are similar (cosine similarity: 1.00)\n",
      "DOB '05/09/1984' and DOB '45/67' are similar (cosine similarity: 0.99)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# List of DOB strings in the corpus\n",
    "corpus = [\n",
    "    \"20/03/98\",\n",
    "    \"12/04\",\n",
    "    \"05/09/1984\",\n",
    "    \"45/67\"\n",
    "]\n",
    "\n",
    "# Function to mask a DOB string (replace digits with 'number' and other characters with 'special')\n",
    "def mask_dob(dob):\n",
    "    return ' '.join(['number' if c.isdigit() else 'special' for c in dob])\n",
    "\n",
    "# Step 1: Mask the DOBs in the corpus\n",
    "masked_corpus = [mask_dob(dob) for dob in corpus]\n",
    "\n",
    "# Step 2: Convert the masked DOB string into spaCy token embeddings\n",
    "def get_token_embeddings(dob):\n",
    "    doc = nlp(dob)\n",
    "    return [token.vector for token in doc]\n",
    "\n",
    "# Step 3: Calculate average embeddings for 'number' and 'special' separately\n",
    "def calculate_avg_embeddings(masked_dob):\n",
    "    doc = nlp(masked_dob)\n",
    "    number_embeddings = []\n",
    "    special_embeddings = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text == \"number\":\n",
    "            number_embeddings.append(token.vector)\n",
    "        elif token.text == \"special\":\n",
    "            special_embeddings.append(token.vector)\n",
    "    \n",
    "    # Calculate the average embeddings for 'number' and 'special'\n",
    "    number_avg_vector = np.mean(number_embeddings, axis=0) if number_embeddings else np.zeros_like(doc[0].vector)\n",
    "    special_avg_vector = np.mean(special_embeddings, axis=0) if special_embeddings else np.zeros_like(doc[0].vector)\n",
    "\n",
    "    # Combine the vectors\n",
    "    return np.concatenate([number_avg_vector, special_avg_vector])\n",
    "\n",
    "# Step 4: Compute the combined vector representation for each DOB\n",
    "dob_vectors = [calculate_avg_embeddings(masked_dob) for masked_dob in masked_corpus]\n",
    "\n",
    "# Step 5: Calculate cosine similarity between the vectors\n",
    "similarity_matrix = cosine_similarity(dob_vectors)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Step 6: Check for similarity threshold to determine if two DOBs are similar\n",
    "threshold = 0.8  # Define a threshold for considering DOBs as similar\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for j in range(i+1, len(corpus)):  # Avoid comparing a DOB with itself\n",
    "        similarity = similarity_matrix[i][j]\n",
    "        if similarity > threshold:\n",
    "            print(f\"DOB '{corpus[i]}' and DOB '{corpus[j]}' are similar (cosine similarity: {similarity:.2f})\")\n",
    "        else:\n",
    "            print(f\"DOB '{corpus[i]}' and DOB '{corpus[j]}' are not similar (cosine similarity: {similarity:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61011516-bae5-4f42-9d04-c4ff98defd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.0000001  0.9946969  0.9961081  0.9917478  0.9946969  0.6305121 ]\n",
      " [0.9946969  1.0000001  0.9880272  0.9859574  1.0000001  0.6342239 ]\n",
      " [0.9961081  0.9880272  1.0000001  0.9941449  0.9880272  0.6489448 ]\n",
      " [0.9917478  0.9859574  0.9941449  0.9999999  0.9859574  0.66352135]\n",
      " [0.9946969  1.0000001  0.9880272  0.9859574  1.0000001  0.6342239 ]\n",
      " [0.6305121  0.6342239  0.6489448  0.66352135 0.6342239  1.0000001 ]]\n",
      "DOB '20/03/98' and DOB '12/04' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '05/09/1984' are similar (cosine similarity: 1.00)\n",
      "DOB '20/03/98' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '45/67' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '7882899067' are not similar (cosine similarity: 0.63)\n",
      "DOB '12/04' and DOB '05/09/1984' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '45/67' are similar (cosine similarity: 1.00)\n",
      "DOB '12/04' and DOB '7882899067' are not similar (cosine similarity: 0.63)\n",
      "DOB '05/09/1984' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '05/09/1984' and DOB '45/67' are similar (cosine similarity: 0.99)\n",
      "DOB '05/09/1984' and DOB '7882899067' are not similar (cosine similarity: 0.65)\n",
      "DOB '30/5/2003' and DOB '45/67' are similar (cosine similarity: 0.99)\n",
      "DOB '30/5/2003' and DOB '7882899067' are not similar (cosine similarity: 0.66)\n",
      "DOB '45/67' and DOB '7882899067' are not similar (cosine similarity: 0.63)\n",
      "DOB '20/03/98' is not valid.\n",
      "DOB '12/04' is not valid.\n",
      "DOB '05/09/1984' is valid.\n",
      "DOB '30/5/2003' is valid.\n",
      "DOB '45/67' is not valid.\n",
      "DOB '7882899067' is not valid.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# List of DOB strings in the corpus\n",
    "corpus = [\n",
    "    \"20/03/98\",\n",
    "    \"12/04\",\n",
    "    \"05/09/1984\",\n",
    "    \"30/5/2003\",\n",
    "    \"45/67\",\n",
    "    \"7882899067\"\n",
    "]\n",
    "\n",
    "# Function to mask a DOB string (replace digits with 'number' and other characters with 'special')\n",
    "def mask_dob(dob):\n",
    "    return ' '.join(['number' if c.isdigit() else 'special' for c in dob])\n",
    "\n",
    "# Function to check if a given date is valid\n",
    "def is_valid_date(dob):\n",
    "    try:\n",
    "        # Attempt to parse the DOB with different formats\n",
    "        if len(dob.split(\"/\")) == 2:  # e.g., \"12/04\"\n",
    "            dob = dob + \"/01\"  # Add a dummy year to validate the day and month only\n",
    "        # Try parsing the date with year, month, day\n",
    "        parsed_date = datetime.strptime(dob, \"%d/%m/%Y\")\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Step 1: Mask the DOBs in the corpus\n",
    "masked_corpus = [mask_dob(dob) for dob in corpus]\n",
    "\n",
    "# Step 2: Convert the masked DOB string into spaCy token embeddings\n",
    "def get_token_embeddings(dob):\n",
    "    doc = nlp(dob)\n",
    "    return [token.vector for token in doc]\n",
    "\n",
    "# Step 3: Calculate average embeddings for 'number' and 'special' separately\n",
    "def calculate_avg_embeddings(masked_dob):\n",
    "    doc = nlp(masked_dob)\n",
    "    number_embeddings = []\n",
    "    special_embeddings = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text == \"number\":\n",
    "            number_embeddings.append(token.vector)\n",
    "        elif token.text == \"special\":\n",
    "            special_embeddings.append(token.vector)\n",
    "    \n",
    "    # Calculate the average embeddings for 'number' and 'special'\n",
    "    number_avg_vector = np.mean(number_embeddings, axis=0) if number_embeddings else np.zeros_like(doc[0].vector)\n",
    "    special_avg_vector = np.mean(special_embeddings, axis=0) if special_embeddings else np.zeros_like(doc[0].vector)\n",
    "\n",
    "    # Combine the vectors\n",
    "    return np.concatenate([number_avg_vector, special_avg_vector])\n",
    "\n",
    "# Step 4: Compute the combined vector representation for each DOB\n",
    "dob_vectors = [calculate_avg_embeddings(masked_dob) for masked_dob in masked_corpus]\n",
    "\n",
    "# Step 5: Calculate cosine similarity between the vectors\n",
    "similarity_matrix = cosine_similarity(dob_vectors)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix)\n",
    "\n",
    "# Step 6: Check for similarity threshold to determine if two DOBs are similar\n",
    "threshold = 0.8  # Define a threshold for considering DOBs as similar\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    for j in range(i+1, len(corpus)):  # Avoid comparing a DOB with itself\n",
    "        similarity = similarity_matrix[i][j]\n",
    "        if similarity > threshold:\n",
    "            print( f\"DOB '{corpus[i]}' and DOB '{corpus[j]}' are similar (cosine similarity: {similarity:.2f})\")\n",
    "        else:\n",
    "            print(f\"DOB '{corpus[i]}' and DOB '{corpus[j]}' are not similar (cosine similarity: {similarity:.2f})\")\n",
    "\n",
    "# Step 7: Check if the DOB is valid\n",
    "for dob in corpus:\n",
    "    if is_valid_date(dob):\n",
    "        print( f\"DOB '{dob}' is valid.\")\n",
    "    else:\n",
    "        print(f\"DOB '{dob}' is not valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247c34e-a543-4642-b026-3a839e59c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Final code with valid DOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2970cc71-6744-4f9e-89ce-cc9903713dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOB '45/67' is invalid and skipped.\n",
      "DOB '7882899067' is invalid and skipped.\n",
      "DOB '29/02/2001' is invalid and skipped.\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "['1.00', '0.99', '1.00', '0.99', '1.00']\n",
      "['0.99', '1.00', '0.99', '0.99', '0.99']\n",
      "['1.00', '0.99', '1.00', '0.99', '1.00']\n",
      "['0.99', '0.99', '0.99', '1.00', '0.99']\n",
      "['1.00', '0.99', '1.00', '0.99', '1.00']\n",
      "\n",
      "Similar DOBs based on cosine similarity:\n",
      "DOB '20/03/98' and DOB '12/04' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '05/09/1984' are similar (cosine similarity: 1.00)\n",
      "DOB '20/03/98' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '20/03/98' and DOB '29/02/2000' are similar (cosine similarity: 1.00)\n",
      "DOB '12/04' and DOB '05/09/1984' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '12/04' and DOB '29/02/2000' are similar (cosine similarity: 0.99)\n",
      "DOB '05/09/1984' and DOB '30/5/2003' are similar (cosine similarity: 0.99)\n",
      "DOB '05/09/1984' and DOB '29/02/2000' are similar (cosine similarity: 1.00)\n",
      "DOB '30/5/2003' and DOB '29/02/2000' are similar (cosine similarity: 0.99)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the pre-trained spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# List of DOB strings in the corpus\n",
    "corpus = [\n",
    "    \"20/03/98\",\n",
    "    \"12/04\",\n",
    "    \"05/09/1984\",\n",
    "    \"30/5/2003\",\n",
    "    \"45/67\",  # Invalid\n",
    "    \"7882899067\",  # Invalid\n",
    "    \"29/02/2000\",  # Valid Leap Year\n",
    "    \"29/02/2001\"   # Invalid (non-leap year)\n",
    "]\n",
    "\n",
    "# Function to mask a DOB string (replace digits with 'number' and other characters with 'special')\n",
    "def mask_dob(dob):\n",
    "    return ' '.join(['number' if c.isdigit() else 'special' for c in dob])\n",
    "\n",
    "# Function to check if a given date is valid\n",
    "def is_valid_date(dob):\n",
    "    try:\n",
    "        parts = dob.split(\"/\")\n",
    "        if len(parts) == 2:  # e.g., \"12/04\"\n",
    "            day, month = int(parts[0]), int(parts[1])\n",
    "            year = 2000  # Dummy leap year for validation\n",
    "        elif len(parts) == 3:  # e.g., \"20/03/98\" or \"05/09/1984\"\n",
    "            day, month = int(parts[0]), int(parts[1])\n",
    "            year = int(parts[2]) if len(parts[2]) == 4 else int(\"20\" + parts[2])\n",
    "        else:\n",
    "            return False  # Invalid format\n",
    "\n",
    "        # Check month validity\n",
    "        if not (1 <= month <= 12):\n",
    "            return False\n",
    "\n",
    "        # Days in each month\n",
    "        days_in_month = {\n",
    "            1: 31, 2: 28, 3: 31, 4: 30, 5: 31, 6: 30,\n",
    "            7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31\n",
    "        }\n",
    "\n",
    "        # Adjust February for leap years\n",
    "        if month == 2 and ((year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)):\n",
    "            days_in_month[2] = 29\n",
    "\n",
    "        # Check day validity\n",
    "        return 1 <= day <= days_in_month[month]\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# Step 1: Mask the DOBs in the corpus\n",
    "masked_corpus = [mask_dob(dob) for dob in corpus]\n",
    "\n",
    "# # Process the corpus\n",
    "# masked_corpus = [mask_dob(dob) for dob in corpus]\n",
    "# print(\"\\nMasked Corpus:\", masked_corpus)\n",
    "\n",
    "# Step 2: Convert the masked DOB string into spaCy token embeddings\n",
    "def get_token_embeddings(dob):\n",
    "    doc = nlp(dob)\n",
    "    return [token.vector for token in doc]\n",
    "\n",
    "# Step 3: Calculate average embeddings for 'number' and 'special' separately\n",
    "def calculate_avg_embeddings(masked_dob):\n",
    "    doc = nlp(masked_dob)\n",
    "    number_embeddings = []\n",
    "    special_embeddings = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text == \"number\":\n",
    "            number_embeddings.append(token.vector)\n",
    "        elif token.text == \"special\":\n",
    "            special_embeddings.append(token.vector)\n",
    "    \n",
    "    # Calculate the average embeddings for 'number' and 'special'\n",
    "    number_avg_vector = np.mean(number_embeddings, axis=0) if number_embeddings else np.zeros_like(doc[0].vector)\n",
    "    special_avg_vector = np.mean(special_embeddings, axis=0) if special_embeddings else np.zeros_like(doc[0].vector)\n",
    "\n",
    "    # Combine the vectors\n",
    "    return np.concatenate([number_avg_vector, special_avg_vector])\n",
    "\n",
    "# Step 4: Compute the combined vector representation for each valid DOB\n",
    "dob_vectors = []\n",
    "valid_dobs = []\n",
    "\n",
    "for i, dob in enumerate(corpus):\n",
    "    if is_valid_date(dob):  # Apply DOB constraints here\n",
    "        vector = calculate_avg_embeddings(masked_corpus[i])\n",
    "        dob_vectors.append(vector)\n",
    "        valid_dobs.append(dob)\n",
    "    else:\n",
    "        print(f\"DOB '{dob}' is invalid and skipped.\")\n",
    "\n",
    "# Step 5: Calculate cosine similarity between the vectors\n",
    "if dob_vectors:\n",
    "    similarity_matrix = cosine_similarity(dob_vectors)\n",
    "\n",
    "    # Print the cosine similarity matrix\n",
    "    print(\"\\nCosine Similarity Matrix:\")\n",
    "    for row in similarity_matrix:\n",
    "        print([\"{:.2f}\".format(x) for x in row])\n",
    "\n",
    "    # Step 6: Check for similarity threshold to determine if two DOBs are similar\n",
    "    threshold = 0.8 # Define a threshold for considering DOBs as similar\n",
    "    print(\"\\nSimilar DOBs based on cosine similarity:\")\n",
    "    for i in range(len(valid_dobs)):\n",
    "        for j in range(i+1, len(valid_dobs)):  # Avoid comparing a DOB with itself\n",
    "            similarity = similarity_matrix[i][j]\n",
    "            if similarity > threshold:\n",
    "                print(f\"DOB '{valid_dobs[i]}' and DOB '{valid_dobs[j]}' are similar (cosine similarity: {similarity:.2f})\")\n",
    "else:\n",
    "    print(\"No valid DOB vectors found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb45d546-243e-4c5f-a978-616c07679eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.10-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl.metadata (172 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting setuptools (from spacy)\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in ./myenv/lib/python3.12/site-packages (from spacy) (2.1.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.17.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.8.2-cp312-cp312-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.10-cp312-cp312-macosx_11_0_arm64.whl (42 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.11-cp312-cp312-macosx_11_0_arm64.whl (27 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-macosx_11_0_arm64.whl (128 kB)\n",
      "Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "Downloading pydantic_core-2.27.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp312-cp312-macosx_11_0_arm64.whl (486 kB)\n",
      "Downloading thinc-8.3.2-cp312-cp312-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.0/761.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp312-cp312-macosx_14_0_arm64.whl (5.0 MB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.0.1-cp312-cp312-macosx_11_0_arm64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-3.0.2-cp312-cp312-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (174 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.17.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, shellingham, setuptools, pygments, numpy, murmurhash, mdurl, MarkupSafe, idna, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 idna-3.10 jinja2-3.1.4 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 numpy-2.0.2 preshed-3.0.9 pydantic-2.10.3 pydantic-core-2.27.1 pygments-2.18.0 requests-2.32.3 rich-13.9.4 setuptools-75.6.0 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 tqdm-4.67.1 typer-0.15.1 typing-extensions-4.12.2 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bf2dbc7-839a-4a58-ba70-61b4a21a1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
