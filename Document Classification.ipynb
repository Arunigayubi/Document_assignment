{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3f1e8d-5af7-4d98-9741-a21682a14406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d078ed-5e60-4feb-95c0-84ad77bb76c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (from pdf2image) (10.4.0)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pdf2image\n",
      "Successfully installed pdf2image-1.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3102bb6b-43c5-454e-9d4f-f19fd460aed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Using cached PyMuPDF-1.24.14-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Using cached PyMuPDF-1.24.14-cp39-abi3-macosx_11_0_arm64.whl (18.4 MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d7be62c-f243-4472-8e36-fcc2937c0d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "\n",
    "images = convert_from_path(\"/Users/aruniga.baskaran/Downloads/Mixed Multipage Document.pdf\", dpi=300)\n",
    "for i, img in enumerate(images):\n",
    "    img.save(f\"page_{i + 1}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0bd117-01e5-4532-b512-59994557087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "resized_images = []\n",
    "for i in range(len(images)):\n",
    "    image = cv2.imread(f'page_{i + 1}.png', cv2.IMREAD_GRAYSCALE)\n",
    "    resized = cv2.resize(image, (50, 50))  # Reduce resolution to 50x50 pixels\n",
    "    resized_images.append(resized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3f93abe-31a4-4024-8e88-611ecf5e188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "flattened_images = [img.flatten() for img in resized_images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba5c3a2-cdf9-469e-a438-614b3828fd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8),\n",
       " array([255, 255, 255, ..., 255, 255, 255], dtype=uint8)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a4d457b-aa94-4db1-84a7-ba832e7ff64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(image1, image2):\n",
    "    return np.mean((image1 - image2) ** 2)\n",
    "\n",
    "similarities = []\n",
    "for i in range(len(flattened_images) - 1):\n",
    "    similarity = mse(flattened_images[i], flattened_images[i + 1])\n",
    "    similarities.append(similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "762104e7-b797-4f86-8c98-569c396b8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoints = []\n",
    "threshold = 500  # Adjust based on testing\n",
    "for i, similarity in enumerate(similarities):\n",
    "    if similarity > threshold:\n",
    "        breakpoints.append(i + 1)  # Page i+1 starts a new document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a47ce0a2-619c-48e0-a835-72543a78aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "start = 0\n",
    "for bp in breakpoints:\n",
    "    documents.append(range(start, bp))\n",
    "    start = bp\n",
    "documents.append(range(start, len(flattened_images)))  # Last document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70488-4095-452d-a1df-4f933b11ed71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c357bf28-2122-4283-a35e-6117f2ff833a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "853408b9-f836-4f8d-8f45-48ff82d50d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: Start Page = 1, End Page = 7\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de725366-3738-4c67-b255-35b60c108797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf # imports the pymupdf library\n",
    "doc = pymupdf.open(\"/Users/aruniga.baskaran/Downloads/Mixed Multipage Document.pdf\") # open a document\n",
    "for page in doc: # iterate the document pages\n",
    "  text = page.get_text() # get plain text encoded as UTF-8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb8b8530-b0e2-4012-9398-eb10ba29b547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ ---------\n",
      "annotated-types    0.7.0\n",
      "blis               1.0.1\n",
      "catalogue          2.0.10\n",
      "certifi            2024.8.30\n",
      "charset-normalizer 3.4.0\n",
      "click              8.1.7\n",
      "cloudpathlib       0.20.0\n",
      "confection         0.1.5\n",
      "cymem              2.0.10\n",
      "en_core_web_sm     3.8.0\n",
      "idna               3.10\n",
      "Jinja2             3.1.4\n",
      "langcodes          3.5.0\n",
      "language_data      1.3.0\n",
      "marisa-trie        1.2.1\n",
      "markdown-it-py     3.0.0\n",
      "MarkupSafe         3.0.2\n",
      "mdurl              0.1.2\n",
      "murmurhash         1.0.11\n",
      "numpy              2.0.2\n",
      "opencv-python      4.10.0.84\n",
      "packaging          24.2\n",
      "pdf2image          1.17.0\n",
      "pillow             11.0.0\n",
      "pip                24.2\n",
      "preshed            3.0.9\n",
      "pydantic           2.10.3\n",
      "pydantic_core      2.27.1\n",
      "Pygments           2.18.0\n",
      "PyMuPDF            1.24.14\n",
      "pytesseract        0.3.13\n",
      "requests           2.32.3\n",
      "rich               13.9.4\n",
      "setuptools         75.6.0\n",
      "shellingham        1.5.4\n",
      "smart-open         7.0.5\n",
      "spacy              3.8.2\n",
      "spacy-legacy       3.0.12\n",
      "spacy-loggers      1.0.5\n",
      "srsly              2.4.8\n",
      "thinc              8.3.2\n",
      "tqdm               4.67.1\n",
      "typer              0.15.1\n",
      "typing_extensions  4.12.2\n",
      "urllib3            2.2.3\n",
      "wasabi             1.1.3\n",
      "weasel             0.4.1\n",
      "wrapt              1.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc662eac-59d2-4da2-81d3-02b2feb9ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## k-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce6640d-406f-477d-8c4f-756b92f4ce85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Type 1: Pages [2, 3]\n",
      "Document Type 2: Pages [4, 5, 6, 7]\n",
      "Document Type 3: Pages [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pdf2image import convert_from_path\n",
    "from sklearn.cluster import KMeans\n",
    "import cv2\n",
    "\n",
    "# Step 1: Convert PDF to Images\n",
    "pdf_path = \"/Users/aruniga.baskaran/Downloads/Mixed Multipage Document.pdf\"\n",
    "images = convert_from_path(pdf_path, dpi=100)\n",
    "\n",
    "# Step 2: Preprocess Images (Resize and Flatten)\n",
    "processed_images = []\n",
    "for img in images:\n",
    "    img_resized = img.resize((50, 50))  # Resize to 50x50 for faster comparison\n",
    "    img_array = np.array(img_resized).flatten()  # Flatten into a 1D array\n",
    "    processed_images.append(img_array)\n",
    "\n",
    "processed_images = np.array(processed_images)\n",
    "\n",
    "# Step 3: Apply Clustering Algorithm\n",
    "num_clusters = 3  # Adjust based on the number of document types\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(processed_images)\n",
    "\n",
    "# Step 4: Group Pages by Clusters\n",
    "clusters = {i: [] for i in range(num_clusters)}\n",
    "for page_num, label in enumerate(labels):\n",
    "    clusters[label].append(page_num + 1)  # Convert to 1-based page indexing\n",
    "\n",
    "# Step 5: Output the Groups\n",
    "for cluster_id, pages in clusters.items():\n",
    "    print(f\"Document Type {cluster_id + 1}: Pages {pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0350f11b-b5b2-47f7-b3d2-35c7284d9dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped page ranges based on visual layout:\n",
      "Group 1: Start Page 1, End Page 3\n",
      "Group 2: Start Page 4, End Page 7\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def group_pages_by_visual_layout(pdf_path, resolution=150, similarity_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Group similar pages in a PDF based on their visual layout.\n",
    "\n",
    "    Parameters:\n",
    "    - pdf_path (str): Path to the input PDF file.\n",
    "    - resolution (int): DPI for rendering pages into images.\n",
    "    - similarity_threshold (float): Threshold for grouping similar pages (0-1 scale).\n",
    "\n",
    "    Returns:\n",
    "    - List of grouped page ranges based on visual similarity.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    # Render pages into pixmaps and convert them to feature arrays\n",
    "    page_features = []\n",
    "    for page_num in range(len(doc)):\n",
    "        pixmap = doc[page_num].get_pixmap(dpi=resolution)\n",
    "        # Normalize pixel data to reduce size and focus on layout\n",
    "        img = np.frombuffer(pixmap.samples, dtype=np.uint8).astype(float)\n",
    "        img = img / 255.0  # Normalize pixel values to [0, 1]\n",
    "        page_features.append(img)\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    # Compare page features and calculate similarity matrix\n",
    "    n_pages = len(page_features)\n",
    "    similarity_matrix = np.zeros((n_pages, n_pages))\n",
    "\n",
    "    for i in range(n_pages):\n",
    "        for j in range(i, n_pages):\n",
    "            sim = cosine_similarity(\n",
    "                page_features[i].reshape(1, -1), page_features[j].reshape(1, -1)\n",
    "            )[0, 0]\n",
    "            similarity_matrix[i, j] = sim\n",
    "            similarity_matrix[j, i] = sim\n",
    "\n",
    "    # Group pages based on similarity\n",
    "    groups = []\n",
    "    visited = set()\n",
    "\n",
    "    for i in range(n_pages):\n",
    "        if i in visited:\n",
    "            continue\n",
    "        group = [i + 1]  # Pages are 1-indexed\n",
    "        visited.add(i)\n",
    "        for j in range(i + 1, n_pages):\n",
    "            if j not in visited and similarity_matrix[i, j] >= similarity_threshold:\n",
    "                group.append(j + 1)\n",
    "                visited.add(j)\n",
    "        groups.append(group)\n",
    "\n",
    "    # Merge groups into page ranges\n",
    "    grouped_ranges = []\n",
    "    for group in groups:\n",
    "        grouped_ranges.append((group[0], group[-1]))\n",
    "\n",
    "    return grouped_ranges\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"/Users/aruniga.baskaran/Downloads/Mixed Multipage Document.pdf\"  # Replace with your input PDF file\n",
    "groups = group_pages_by_visual_layout(pdf_path)\n",
    "\n",
    "# Display results\n",
    "print(\"Grouped page ranges based on visual layout:\")\n",
    "for group_id, (start, end) in enumerate(groups, start=1):\n",
    "    print(f\"Group {group_id}: Start Page {start}, End Page {end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3987cab4-8d69-400a-be22-a72982f86070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying page 1\n",
      "Document has similar intensity on both sides (Aadhaar/PAN likely)\n",
      "Extracted Text: \n",
      "Document is of unknown type\n",
      "Classifying page 2\n",
      "Document has similar intensity on both sides (Aadhaar/PAN likely)\n",
      "Extracted Text:  GOVERNMENTOFINDIC,\n",
      "aa GM\n",
      "\n",
      "Salim Khan\n",
      "\n",
      "wy fafel/DOB: 01/01/1998\n",
      "es/ MALE\n",
      "\n",
      "Mobile No: 9555679554\n",
      "\n",
      "5475 7089 7656 >\n",
      "\n",
      "VID : 9195 7699 1241 4064\n",
      "\n",
      "Issue Date: 19/01/2018\n",
      "\n",
      "Download Date: 22/12/2020\n",
      "\n",
      "\n",
      "Document is Aadhaar\n",
      "Classifying page 3\n",
      "Document has similar intensity on both sides (Aadhaar/PAN likely)\n",
      "Extracted Text: Batra\n",
      "\n",
      ", OR\n",
      "- 110074\n",
      "\n",
      "Address :\n",
      "\n",
      "S/O Abdur Rahim, F-70-C, KH NO-584,\n",
      "CHHATTARPUR EXTN, Chattar Pur, South\n",
      "Delhi,\n",
      "\n",
      "Delhi - 110074\n",
      "\n",
      "5475 7089 7656\n",
      "VID : 9195 7699 1241 4064\n",
      "\n",
      "\n",
      "Document is Aadhaar\n",
      "Classifying page 4\n",
      "Document has different intensity on both sides (Not Aadhaar/PAN)\n",
      "Classifying page 5\n",
      "Document has different intensity on both sides (Not Aadhaar/PAN)\n",
      "Classifying page 6\n",
      "Document has different intensity on both sides (Not Aadhaar/PAN)\n",
      "Classifying page 7\n",
      "Document has different intensity on both sides (Not Aadhaar/PAN)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import re\n",
    "\n",
    "# Convert PDF to images\n",
    "def pdf_to_images(pdf_path):\n",
    "    pages = convert_from_path(pdf_path, 300)  # Convert with 300 DPI for better quality\n",
    "    return [np.array(page) for page in pages]  # Convert PIL images to NumPy arrays\n",
    "\n",
    "# Preprocess image: Convert to grayscale and enhance for OCR\n",
    "def preprocess_image(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert image to grayscale\n",
    "    # Enhance contrast and remove noise to improve OCR accuracy\n",
    "    gray_image = cv2.equalizeHist(gray_image)  # Histogram equalization for contrast enhancement\n",
    "    return gray_image\n",
    "\n",
    "# Compare pixel intensity similarity using SSIM\n",
    "def compare_intensity_similarity(top_half, bottom_half):\n",
    "    similarity_index, _ = ssim(top_half, bottom_half, full=True)  # SSIM between top and bottom halves\n",
    "    return similarity_index\n",
    "\n",
    "# Compare mean pixel intensities\n",
    "def compare_mean_intensity(top_half, bottom_half):\n",
    "    mean_top = np.mean(top_half)\n",
    "    mean_bottom = np.mean(bottom_half)\n",
    "    intensity_diff = np.abs(mean_top - mean_bottom)\n",
    "    return intensity_diff\n",
    "\n",
    "# Extract text from image using OCR\n",
    "def extract_text(image):\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    print(\"Extracted Text:\", text)  # Debug: Print the extracted text to inspect\n",
    "    return text\n",
    "\n",
    "# Check if the text matches Aadhaar format\n",
    "def is_aadhaar(text):\n",
    "    aadhaar_pattern = r'\\d{4}\\s\\d{4}\\s\\d{4}'  # Aadhaar format: 0000 0000 0000\n",
    "    return bool(re.search(aadhaar_pattern, text))\n",
    "\n",
    "# Check if the text matches PAN format\n",
    "def is_pan(text):\n",
    "    # Improved PAN regex pattern for better matching\n",
    "    pan_pattern = r'[A-Z]{5}[0-9]{4}[A-Z]{1}'  # PAN format: AAAAA1234A\n",
    "    return bool(re.search(pan_pattern, text))\n",
    "\n",
    "# Function to classify the document as Aadhaar or PAN\n",
    "def classify_document(image):\n",
    "    # Step 1: Preprocess the image\n",
    "    gray_image = preprocess_image(image)\n",
    "\n",
    "    # Step 2: Divide the image into two halves (top and bottom)\n",
    "    height, width = gray_image.shape\n",
    "\n",
    "    # Ensure both halves are of the same size by adjusting if height is odd\n",
    "    half_height = height // 2\n",
    "    if height % 2 != 0:  # If the height is odd, adjust to ensure even split\n",
    "        half_height += 1\n",
    "\n",
    "    top_half = gray_image[:half_height, :]\n",
    "    bottom_half = gray_image[half_height:, :]\n",
    "\n",
    "    # Resize bottom half to match top half in case the height was adjusted\n",
    "    bottom_half = cv2.resize(bottom_half, (top_half.shape[1], top_half.shape[0]))\n",
    "\n",
    "    # Step 3: Compare pixel intensity using SSIM\n",
    "    ssim_similarity = compare_intensity_similarity(top_half, bottom_half)\n",
    "\n",
    "    # Step 4: Compare mean pixel intensities (alternative to SSIM)\n",
    "    mean_intensity_diff = compare_mean_intensity(top_half, bottom_half)\n",
    "\n",
    "    # Step 5: Classification based on threshold\n",
    "    ssim_threshold = 0.9  # Threshold for SSIM similarity\n",
    "    mean_intensity_threshold = 10  # Threshold for mean intensity difference\n",
    "\n",
    "    if ssim_similarity > ssim_threshold or mean_intensity_diff < mean_intensity_threshold:\n",
    "        print(\"Document has similar intensity on both sides (Aadhaar/PAN likely)\")\n",
    "\n",
    "        # Step 6: Extract text from the image\n",
    "        text = extract_text(image)\n",
    "\n",
    "        # Step 7: Further classify based on text\n",
    "        if is_aadhaar(text):\n",
    "            print(\"Document is Aadhaar\")\n",
    "        elif is_pan(text):\n",
    "            print(\"Document is PAN\")\n",
    "        else:\n",
    "            print(\"Document is of unknown type\")\n",
    "    else:\n",
    "        print(\"Document has different intensity on both sides (Not Aadhaar/PAN)\")\n",
    "\n",
    "# Main function to process PDF and classify\n",
    "def process_and_classify_pdf(pdf_path):\n",
    "    images = pdf_to_images(pdf_path)  # Convert PDF pages to images\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        print(f\"Classifying page {i+1}\")\n",
    "        classify_document(image)\n",
    "\n",
    "# Example usage\n",
    "pdf_path = '/Users/aruniga.baskaran/Downloads/Mixed Multipage Document.pdf'  # Path to the PDF document\n",
    "process_and_classify_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab754f6a-52c1-4eff-bfe0-1ef7c38a776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02ac78f3-02f0-4410-9766-0e5cfdff965f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./myenv/lib/python3.12/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3905a4-57ed-49f1-816d-f6fbb10dd3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/numpy/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.3\n",
      "    Uninstalling numpy-2.1.3:\n",
      "      Successfully uninstalled numpy-2.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2251cf5-3689-4535-a279-246a76fa854f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: scikit-learn 1.5.2\n",
      "Uninstalling scikit-learn-1.5.2:\n",
      "  Successfully uninstalled scikit-learn-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc17525c-2738-491c-9be6-115f0d1b7243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Downloading numpy-2.1.3-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m180.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 2.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf6925d-ebe6-48bf-8599-42d7bb89b433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./myenv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4506b78-a328-4920-9194-c2c5e9849eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./myenv/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: scipy in ./myenv/lib/python3.12/site-packages (1.14.1)\n",
      "Requirement already satisfied: matplotlib in ./myenv/lib/python3.12/site-packages (3.9.3)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./myenv/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./myenv/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./myenv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./myenv/lib/python3.12/site-packages (from matplotlib) (4.55.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./myenv/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in ./myenv/lib/python3.12/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./myenv/lib/python3.12/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./myenv/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn scipy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380daec1-adf4-4d89-a5dc-3e01c81c300f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba937f0-fb65-44a6-8ad0-ba14481b800d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
